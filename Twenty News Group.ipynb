{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import operator\n",
    "from gensim.summarization.summarizer import summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading training dataset and testing dataset\n",
    "Notice: since the text are emails so they have \"From\", \"Subjects\", \"Lines\", etc. Set arguments remove to remove news group header, blocks at the ends of posts that look like signatures and lines that appear to be quoting another post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(subset='train', shuffle=True, remove = ('headers','footers','quotes'))\n",
    "test = fetch_20newsgroups(subset='test', shuffle=True, remove = ('headers','footers','quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR', 'description'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}\n"
     ]
    }
   ],
   "source": [
    "print(train.target_names)\n",
    "target_set = set(train.target)\n",
    "print(target_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the lebel of messages are stored in target names and are represented by integers from 0 to 19 when fitting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is clean without header, footer or quoting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean the text.\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean(text):\n",
    "    text = text.replace(\"-\", \"\")\n",
    "    tokenized_text = sent_tokenize(text)\n",
    "    tokenized_text_list = []\n",
    "    for sent in tokenized_text:\n",
    "        word_list = word_tokenize(sent)\n",
    "        word_list = [x for x in word_list if x not in string.punctuation]\n",
    "        word_list[len(word_list)-1] = word_list[len(word_list)-1]+'.'\n",
    "        new_sent = \" \".join(word_list)\n",
    "        \n",
    "        \n",
    "        tokenized_text_list.append(new_sent)\n",
    "        \n",
    "    \n",
    "    return(\" \".join(tokenized_text_list))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\nI've had the board for over a year, and it does work with Diskdoubler,\\nbut not with Autodoubler, due to a licensing problem with Stac Technologies,\\nthe owners of the board's compression technology. (I'm writing this\\nfrom memory; I've lost the reference. Please correct me if I'm wrong.)\\n\\nUsing the board, I've had problems with file icons being lost, but it's\\nhard to say whether it's the board's fault or something else; however,\\nif I decompress the troubled file and recompress it without the board,\\nthe icon usually reappears. Because of the above mentioned licensing\\nproblem, the freeware expansion utility DD Expand will not decompress\\na board-compressed file unless you have the board installed.\\n\\nSince Stac has its own product now, it seems unlikely that the holes\\nin Autodoubler/Diskdoubler related to the board will be fixed.\\nWhich is sad, and makes me very reluctant to buy Stac's product since\\nthey're being so stinky. (But hey, that's competition.)\\n-- \""
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.data[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I 've had the board for over a year and it does work with Diskdoubler but not with Autodoubler due to a licensing problem with Stac Technologies the owners of the board 's compression technology. I 'm writing this from memory I 've lost the reference. Please correct me if I 'm wrong. Using the board I 've had problems with file icons being lost but it's hard to say whether it 's the board 's fault or something else however if I decompress the troubled file and recompress it without the board the icon usually reappears. Because of the above mentioned licensing problem the freeware expansion utility DD Expand will not decompress a boardcompressed file unless you have the board installed. Since Stac has its own product now it seems unlikely that the holes in Autodoubler/Diskdoubler related to the board will be fixed. Which is sad and makes me very reluctant to buy Stac 's product since they 're being so stinky. But hey that 's competition.\""
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(train.data[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Text Summarization Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I try to build extractive summarization model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf summarizer  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I built my first sentence ranker by tfidf score. I used tfidf transformer to convert the words in the text into tfidf matrix and store them as a dictionary. For each sentence, I added the tfidf score for all the word in that sentence and then calculate the average. The average score will be the tfidf score for this sentences. I chose the first few sentences when ranking the sentences in a descendant order and generate an extractive summarization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idfvec = TfidfVectorizer()\n",
    "tf_idfvec.fit(train.data)\n",
    "tf_matrix = tf_idfvec.transform(train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the rank for each sentence\n",
    "def get_sentence_score_tf(sentence):\n",
    "    \"\"\"\n",
    "    parameter: sentence- a string representing the sentence \n",
    "    return : the tf-idf score for this sentence \n",
    "    \"\"\"\n",
    "    word_list = word_tokenize(sentence)\n",
    "    n = len(word_list)\n",
    "    score = 0\n",
    "    for word in word_list:\n",
    "        if word in tf_dict.keys():\n",
    "            score += tf_dict[word]\n",
    "    \n",
    "    ave_score = score/n\n",
    "    return ave_score\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tfidf_summarizer(text, prop):\n",
    "    \"\"\"\n",
    "    parameter: text - corpus which is represented by a string input\n",
    "               prop - double between 0 to 1, indicating the proportion of text is the length of summary\n",
    "    return:  string output representing the summarization of the text. \n",
    "    \"\"\"\n",
    "\n",
    "    corpus_matrix = tf_idfvec.transform([text])\n",
    "    sentence_list = sent_tokenize(text)\n",
    "    n = len(sentence_list)\n",
    "    feature_names = tf_idfvec.get_feature_names\n",
    "    \n",
    "    ## get score: https://stackoverflow.com/questions/34449127/sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-document\n",
    "    doc = 0\n",
    "    feature_index = corpus_matrix[doc,:].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [corpus_matrix[doc, x] for x in feature_index])\n",
    "    \n",
    "    tf_dict = dict() # create dictionary \n",
    "    [(i, s) for (i, s) in tfidf_scores]\n",
    "    for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
    "        tf_dict[w] = s\n",
    "    \n",
    "    score_list = dict()\n",
    "    for i in range(n):\n",
    "        ave_score = get_sentence_score_tf(sentence_list[i])\n",
    "        score_list[i] = ave_score\n",
    "        \n",
    "    sorted_tuple = sorted(score_list.items(), key=operator.itemgetter(1), reverse = True)\n",
    "    #print(sorted_tuple)\n",
    "    num_of_sentence = round(prop*n)\n",
    "    #print([x[0] for x in sorted_tuple[0:num_of_sentence]])\n",
    "    result = ''.join(sentence_list[x[0]] for x in sorted_tuple[0:num_of_sentence])\n",
    "    return(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the board I 've had problems with file icons being lost but it's hard to say whether it 's the board 's fault or something else however if I decompress the troubled file and recompress it without the board the icon usually reappears.Because of the above mentioned licensing problem the freeware expansion utility DD Expand will not decompress a boardcompressed file unless you have the board installed.\n"
     ]
    }
   ],
   "source": [
    "print(Tfidf_summarizer(clean(train.data[9]), 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Entity Recognition Summarizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I built my second text summarizer by using name entity recognition. Sentences with more than or equal to 2 NERs would be important and worth including into summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_score_ner(sentence):\n",
    "    \"\"\"\n",
    "    parameter: sentence - a string representing a sentence\n",
    "    return: score - a double representing the NER score for the sentence \n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    chunks = ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "    for c in chunks:\n",
    "        if type(c) == nltk.tree.Tree:\n",
    "            score += 1\n",
    "    return(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NER_summarizer(text):\n",
    "    \"\"\"\n",
    "    parameter: text - corpus which is represented by a string input\n",
    "\n",
    "    return:  string output representing the summarization of the text.\n",
    "    \"\"\"\n",
    "    text = clean(text)\n",
    "    sent_list = sent_tokenize(text)\n",
    "    score_list = []\n",
    "    for i in range(len(sent_list)):\n",
    "        score_list.append(get_sentence_score_ner(sent_list[i]))\n",
    "    \n",
    "    result = [sent for sent in sent_list if score_list[sent_list.index(sent)]>=3]\n",
    "    result = ''.join(sent for sent in result)\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 've had the board for over a year and it does work with Diskdoubler but not with Autodoubler due to a licensing problem with Stac Technologies the owners of the board 's compression technology.\n"
     ]
    }
   ],
   "source": [
    "print(NER_summarizer(train.data[9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextRank Summarizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    " \n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    " \n",
    "# get the english list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    " \n",
    "def build_similarity_matrix(sentences, stopwords=None):\n",
    "    # Create an empty similarity matrix\n",
    "    #sentences = word_tokenize(sentences)\n",
    "    S = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    " \n",
    "            S[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    " \n",
    "    # normalize the matrix row-wise\n",
    "    for idx in range(len(S)):\n",
    "        S[idx] /= S[idx].sum()\n",
    " \n",
    "    return S\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(A, eps=0.0001, d=0.85):\n",
    "    P = np.ones(len(A)) / len(A)\n",
    "    while True:\n",
    "        new_P = np.ones(len(A)) * (1 - d) / len(A) + d * A.T.dot(P)\n",
    "        delta = abs((new_P - P).sum())\n",
    "        if delta <= eps:\n",
    "            return new_P\n",
    "        P = new_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "def textrank(sentences, top_n=5, stopwords=None):\n",
    "    \"\"\"\n",
    "    sentences - corpus which is represented by a string input\n",
    "\n",
    "    top_n - how may sentences the summary should contain\n",
    "    stopwords - a list of stopwords\n",
    "    \"\"\"\n",
    "    S = build_similarity_matrix(sentences, stop_words) \n",
    "    sentence_ranks = pagerank(S)\n",
    " \n",
    "    # Sort the sentence ranks\n",
    "    ranked_sentence_indexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
    "    selected_sentences = sorted(ranked_sentence_indexes[:top_n])\n",
    "    summary = itemgetter(*selected_sentences)(sentences)        \n",
    "    return summary \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "I've had the board for over a year, and it does work with Diskdoubler,\n",
      "but not with Autodoubler, due to a licensing problem with Stac Technologies,\n",
      "the owners of the board's compression technology. (I'm writing this\n",
      "from memory; I've lost the reference. Please correct me if I'm wrong.)\n",
      "\n",
      "Using the board, I've had problems with file icons being lost, but it's\n",
      "hard to say whether it's the board's fault or something else; however,\n",
      "if I decompress the troubled file and recompress it without the board,\n",
      "the icon usually reappears. Because of the above mentioned licensing\n",
      "problem, the freeware expansion utility DD Expand will not decompress\n",
      "a board-compressed file unless you have the board installed.\n",
      "\n",
      "Since Stac has its own product now, it seems unlikely that the holes\n",
      "in Autodoubler/Diskdoubler related to the board will be fixed.\n",
      "Which is sad, and makes me very reluctant to buy Stac's product since\n",
      "they're being so stinky. (But hey, that's competition.)\n",
      "-- \n"
     ]
    }
   ],
   "source": [
    "text = train.data[9]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I 've had the board for over a year and it does work with Diskdoubler but not with Autodoubler due to a licensing problem with Stac Technologies the owners of the board 's compression technology. I 'm writing this from memory I 've lost the reference. Please correct me if I 'm wrong. Using the board I 've had problems with file icons being lost but it's hard to say whether it 's the board 's fault or something else however if I decompress the troubled file and recompress it without the board the icon usually reappears. Because of the above mentioned licensing problem the freeware expansion utility DD Expand will not decompress a boardcompressed file unless you have the board installed. Since Stac has its own product now it seems unlikely that the holes in Autodoubler/Diskdoubler related to the board will be fixed. Which is sad and makes me very reluctant to buy Stac 's product since they 're being so stinky. But hey that 's competition.\""
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(train.data[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the board I 've had problems with file icons being lost but it's hard to say whether it 's the board 's fault or something else however if I decompress the troubled file and recompress it without the board the icon usually reappears.Because of the above mentioned licensing problem the freeware expansion utility DD Expand will not decompress a boardcompressed file unless you have the board installed.\n"
     ]
    }
   ],
   "source": [
    "print(Tfidf_summarizer(clean(train.data[9]), 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 've had the board for over a year and it does work with Diskdoubler but not with Autodoubler due to a licensing problem with Stac Technologies the owners of the board 's compression technology.\n"
     ]
    }
   ],
   "source": [
    "print(NER_summarizer(clean(train.data[9])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summarizing function from gensim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since Stac has its own product now it seems unlikely that the holes in Autodoubler/Diskdoubler related to the board will be fixed.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(clean(train.data[9])))# From gensim package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textrank summarizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sent_tokenize(clean(train.data[9]))\n",
    "text = [word_tokenize(sent) for sent in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I 've had the board for over a year and it does work with Diskdoubler but not with Autodoubler due to a licensing problem with Stac Technologies the owners of the board 's compression technology .\n",
      "2. Using the board I 've had problems with file icons being lost but it 's hard to say whether it 's the board 's fault or something else however if I decompress the troubled file and recompress it without the board the icon usually reappears .\n",
      "3. But hey that 's competition .\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(textrank(text, top_n = 3, stopwords=stopwords.words('english'))):\n",
    "    print(\"%s. %s\" % ((idx + 1), ' '.join(sentence)))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"I ve had the board for over a year and it does work with Diskdoubler but not with autodoubler dut to a licensing problem with Stac Technologies the owners of the borad's compression technology.\" appears twice\n",
    "\n",
    "\"Using the board I 've had problems with file icons being lost but it's hard to say whether it 's the board 's fault or something else however if I decompress the troubled file and recompress it without the board the icon usually reappears. \" appears twice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More to Go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- More about evaluation: How many sentences are important, how many times the first and last few sentences are included, similarity plot.\n",
    "\n",
    "\n",
    "- About identifying importance: how do we define the label. i.e How do we know if the sentence is important. \n",
    "\n",
    "\n",
    "- Compare with more existed models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
